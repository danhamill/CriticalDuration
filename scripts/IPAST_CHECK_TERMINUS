from datetime import time
import pandas as pd
import numpy as np
from pydsstools.heclib.dss import HecDss
from pydsstools.core import TimeSeriesContainer, HecTime
import altair as alt
from altair import datum
import os
from typing import Tuple
alt.data_transformers.disable_max_rows()
alt.renderers.enable("browser")
 
 
def get_flow_in_data(
    get_flow_in_data(dss_file, sf, year, ds_channel_capacity, pathFlowIn, pathFlowOut, pathElev, window)

):
    """
    Extracts and processes hydrological data from a DSS file to analyze critical duration events.
        dss_file (str): Path to the DSS file containing hydrological data.
        sf (float): Scale factor to be applied to the data.
        year (int): Analysis year in the format YYYY.
        ds_channel_capacity (int or float): Downstream channel capacity limit.
        pathFlowIn (str): DSS path for inflow data.
        pathFlowOut (str): DSS path for outflow data.
        pathElev (str): DSS path for elevation data.
        window (tuple): Time window for data extraction (start, end).
        pd.DataFrame: DataFrame containing the inflow hydrograph data.
        float: Maximum volume for the event based on the analysis.
    Raises:
        AssertionError: If the year is not in the format YYYY or the DSS file does not exist.
    Notes:
        - The function calculates the time of peak storage and checks if the downstream channel capacity is exceeded.
        - Event volume is calculated based on the time of peak storage or the time of downstream channel exceedance.
        - Prints diagnostic messages for key events during the analysis.
    dss_file, sf, year, ds_channel_capacity, pathFlowIn, pathFlowOut, pathElev, window
    """
 
    sf = f"{sf:.2f}"
    assert len(str(year)) == 4, "Year must be 4 digit with format YYYY"
    assert os.path.exists(dss_file), "Cannot locate DSS file"
    print(f"{year} Hydrograph, {sf} Scale Factor.....")
 
    flowIn = readDssData(
        dss_file,
        pathFlowIn,
        variable="flow",
        window=window,
    )
    flowOut = readDssData(
        dss_file,
        pathFlowOut,
        variable="flow",
        window=window,
    )
    elev = readDssData(
        dss_file,
        pathElev,
        variable="elev",
        window=window,
    )
 
    time_peak_stor = elev.elev.idxmax()
    print(f"Time of peak Storage {time_peak_stor}")
 
    # Does Channel exceed downstream constraints?
    flowOut = flowOut.loc[flowOut.flow > ds_channel_capacity, :]
    if not flowOut.empty:
        time_exceed = flowOut.index.min()
        print(f"Time of Downstream Channel Exceedance {time_exceed}")
    else:
        time_exceed = None
 
    if time_exceed is None:
        print("Event Volume calculated as peak in storage (normal operations)...")
        vol_event = flowIn.loc[: time_peak_stor.isoformat(), "flow"].sum() * 3600
    else:
        if time_exceed < time_peak_stor:
            print("Exceeded downstream channel capacity before peak storage...")
            vol_event = flowIn.loc[: time_exceed.isoformat(), "flow"].sum() * 3600
        else:
            print("Event Volume calculated as peak in storage...")
            vol_event = flowIn.loc[: time_peak_stor.isoformat(), "flow"].sum() * 3600
 
    return flowIn, time_peak_stor
 
 
def readDssData(dss_file, path, variable, window):
    """
    Function to read dss data and return a dataframe with the data


    """
    fid = HecDss.Open(dss_file)
    ts = fid.read_ts(path, window=window, trim_missing=False)
    times = ts.pytimes
    values = ts.values
    idx = pd.Index(times, name="date")
    tmp = pd.DataFrame(index=idx, data=values.copy(), columns=[variable])
    fid.close()
 
    return tmp
 
 
def calculate_nday_vols(df, time_peak_stor):
 
    max_vols = {}
 
    for n_day in [1, 2, 3, 4]:
 
        met = f"{n_day}".zfill(3) + "-day"
        df.loc[:, met] = df.flow.rolling(n_day * 24).mean()
 
        idx_max = df[met].idxmax()
        max_val = df[met].max()
        print(met, max_val)
        n_day_vol = max_val * 86400 * n_day
 
        begin_date = (idx_max - pd.DateOffset(hours = 24*n_day-1)).isoformat()
        end_date = (time_peak_stor)
        mask = (df.index >= begin_date) & (df.index <= end_date)
 
        v_event_n_day_window = df.loc[mask, "flow"].sum() * 3600
        norm_vol = v_event_n_day_window / n_day_vol
        norm_vol = int(norm_vol * 1000) / 1000        # Mask out all values outside window
        df.loc[(mask), met] = max_val
        df.loc[df[met] != max_val, met] = np.nan
        max_vols.update({met: norm_vol})
 
    df = df.stack()
    df.index.names = ["date", "metric"]
    df.name = "flow"
    df = df.reset_index()
    df.loc[df.metric != "flow", "text"] = df.loc[df.metric != "flow", "metric"].map(
        max_vols
    )
 
    return df
 
 
def plot_volume_window(df, window):
 
    base = (
        alt.Chart(df)
        .mark_line(color="black", strokeWidth=1)
        .encode(
            x=alt.X(
                "date:T").scale(
                    domain=window
                ).axis(
                    title="Date"
            ),
            y=alt.Y(
                "flow").axis(title="Flow [cfs]"
                    ).scale(
                        domain=[0, df.flow.max()])
        ).transform_filter(
            datum.metric == "flow"
        )
    )
 
    rule = (
        alt.Chart(df)
        .mark_bar(height=2)
        .encode(
            x="min(date):T",
            x2="max(date):T",
            y=alt.Y("flow").scale(domain=[0, df.flow.max()]),
            color="metric",
        )
        .transform_filter(datum.metric != "flow")
    )
 
    tex = (
        alt.Chart(df)
        .mark_text(align="left", baseline="middle", dx=10)
        .encode(
            x="max(date):T",
            y=alt.Y(
                "flow",
                aggregate={"argmax": "date"},
            ).scale(
                domain=[0, df.flow.max()]
            ),
            color="metric",
            text=alt.Text("text", format=".0%"),
        )
        .transform_filter(datum.metric != "flow")
    )
 
    return (base + rule + tex).interactive()
 
 
def main(
    dss_file, year, ds_channel_capacity, pathFlowIn, pathFlowOut, pathElev, window, scale_factor
):
    # Get flow in data
    df, time_peak_stor = get_flow_in_data(
        dss_file, scale_factor, year, ds_channel_capacity, pathFlowIn, pathFlowOut, pathElev, window
    )
 
    # Calculate n-day volumes
    df = calculate_nday_vols(df, time_peak_stor)
 
 
 
    plotWindow = [
        df.date.min().date().strftime('%Y-%m-%d'),
        (df.date.max().date()+pd.Timedelta('3Day')).strftime('%Y-%m-%d')
    ]
    # Plot volume window
    vw_plot = plot_volume_window(df, plotWindow)
 
    # Save the plot to png file
    vw_plot.save(
        rf"outputs\Volume_Window_plots\{year}_{scale_factor}_volume_window.png"
    )
 
    df = df.loc[df.metric!='flow',:]
    df.loc[:,'scale_factor'] = scale_factor
 
    return df
 
if __name__ == "__main__":
 
    # Assign arguments to variables
    dss_file = "data\Terminus_Data.dss"
    year = 1967
    ds_channel_capacity = 5500
    window = tuple("01Dec2021 01:00, 05Dec2021 19:00".split(','))
    scale_factors = list(range(5, 201, 5))

    output = pd.DataFrame()
    for collectionID, scale_factor in enumerate(scale_factorptes):
        scale_factor = scale_factor/100
        collectionID +=1
        pathFlowIn = f"//TRM-TRM INFLOW-KAWEAH/FLOW//1HOUR/C:0000{str(collectionID).zfill(2)}|EXISTING C:1967_SDI D:RESSIM-FRA SHIFT/"
        pathFlowOut = f"//TRM-TRM OUTFLOW-KAWEAH/FLOW//1HOUR/C:0000{str(collectionID).zfill(2)}|EXISTING C:1967_SDI D:RESSIM-FRA SHIFT/"
        pathElev = f"//TERMINUS DAM-POOL/ELEV//1HOUR/C:0000{str(collectionID).zfill(2)}|EXISTING C:1967_SDI D:RESSIM-FRA SHIFT/"

        df = main(dss_file, year, ds_channel_capacity, pathFlowIn, pathFlowOut, pathElev, window, scale_factor)
 
        output = pd.concat([output, df])
        
    output.to_excel(rf'outputs\{year}_critical_duration_summary.xlsx')